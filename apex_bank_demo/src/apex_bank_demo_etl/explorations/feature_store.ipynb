{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de282fa4-d37a-40b0-b5e7-8bbf2e692bd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Creating a Common Feature Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "223efba5-9964-4905-96e5-e7a25940e127",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Trial environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2483d69-4e56-474f-8efa-e1e60079800f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-feature-engineering\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1b82cc5-3c3b-4b95-a444-8a74a04fa209",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Optional: Reset state if needed\n",
    "# -------------------------------------------------------------------------\n",
    "feature_table_name = \"apex_bank_demo.analytics.customer_spending_features\"\n",
    "\n",
    "# Drop the table if it exists to ensure we aren't appending to a broken schema\n",
    "print(f\"ðŸ’£ Dropping old table {feature_table_name} to ensure clean slate...\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {feature_table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f417231-400a-4524-9bb8-d388f2faaf8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Connect data from Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e028da71-1678-447a-8649-48dc790a7d5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"apex_bank_demo\"\n",
    "schema = \"analytics\"\n",
    "\n",
    "raw_transactions_df = spark.table(f\"{catalog}.{schema}.transactions_silver\")\n",
    "labels_df = spark.table(f\"{catalog}.{schema}.fraud_labels\")\n",
    "\n",
    "print(f\"Loaded data from {catalog}.{schema}\")\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"Transactions schema: (Verify column names match logic below)\")\n",
    "raw_transactions_df.printSchema()\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"Labels schema:\")\n",
    "labels_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ba77a6c-8669-418e-9328-fc7ed322c756",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Connect most recent transaction from investigation date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05ec4969-d077-44b8-a697-314ae8f1a529",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient, FeatureLookup\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "raw_transactions_df = spark.table(\"apex_bank_demo.analytics.transactions_silver\")\n",
    "\n",
    "def compute_customer_history(transactions_df):\n",
    "    \n",
    "    # Null checks & type casting\n",
    "    df = (\n",
    "        transactions_df\n",
    "        .filter(F.col(\"transaction_timestamp\").isNotNull())\n",
    "        .withColumn(\"transaction_timestamp\", F.col(\"transaction_timestamp\").cast(TimestampType()))\n",
    "        # Convert timestamp to Long (seconds) for Window calculations\n",
    "        .withColumn(\"ts_long\", F.col(\"transaction_timestamp\").cast(\"long\"))\n",
    "    )\n",
    "    \n",
    "    w_7d = (\n",
    "        Window.partitionBy(\"account_id\")\n",
    "        .orderBy(\"ts_long\")\n",
    "        .rangeBetween(-604800, 0) # Look back 7 days (604800 seconds) from current row\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        df\n",
    "        .withColumn(\"avg_txn_7d\", F.avg(\"amount\").over(w_7d))\n",
    "        .withColumn(\"stddev_txn_7d\", F.stddev(\"amount\").over(w_7d))\n",
    "        .withColumn(\"count_txn_7d\", F.count(\"*\").over(w_7d))\n",
    "        .select( # Select only PKs and Features\n",
    "            \"account_id\", \n",
    "            \"transaction_timestamp\", \n",
    "            \"avg_txn_7d\", \n",
    "            \"stddev_txn_7d\", \n",
    "            \"count_txn_7d\"\n",
    "        )\n",
    "        .dropDuplicates([\"account_id\", \"transaction_timestamp\"])\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a9119e3-b3a6-4510-a546-5bc4ca71284d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create feature table in Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b8cc67d-4f2c-4ea3-ace3-d01f408e1238",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_table_name = \"apex_bank_demo.analytics.customer_spending_features\"\n",
    "\n",
    "print(\"Creating feature table with rolling windows...\")\n",
    "fe.create_table(\n",
    "    name=feature_table_name,\n",
    "    primary_keys=[\"account_id\", \"transaction_timestamp\"],\n",
    "    timeseries_columns=\"transaction_timestamp\",\n",
    "    df=compute_customer_history(raw_transactions_df).dropDuplicates(['account_id', 'transaction_timestamp']),\n",
    "    description=\"True 7-day rolling stats\"\n",
    ")\n",
    "\n",
    "ft_count = spark.table(feature_table_name).count()\n",
    "print(f\"Feature table rebuilt. Row count: {ft_count}\")\n",
    "\n",
    "if ft_count == 0:\n",
    "    print(\"Error: The feature table is empty. Check source data for null timestamps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd91bef7-9e05-4cfb-accd-3d0643affb76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Build \"fraud\" and \"non-fraud\" training dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c58cb9b9-1ffe-40bf-9e91-edcf91adebe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "txns = raw_transactions_df.alias(\"t\")\n",
    "lbls = labels_df.alias(\"l\")\n",
    "\n",
    "# Find the transaction that triggered the fraud\n",
    "joined_df = (\n",
    "    txns.join(lbls, on=\"account_id\")\n",
    "    .filter(F.col(\"t.transaction_timestamp\") <= F.col(\"l.investigation_date\"))\n",
    ")\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "window_spec = Window.partitionBy(\"account_id\").orderBy(F.col(\"t.transaction_timestamp\").desc())\n",
    "\n",
    "fraud_training_df = (\n",
    "    joined_df\n",
    "    .withColumn(\"rank\", F.row_number().over(window_spec))\n",
    "    .filter(F.col(\"rank\") == 1)\n",
    "    .select(\n",
    "        F.col(\"account_id\"),\n",
    "        F.col(\"transaction_timestamp\").cast(TimestampType()),\n",
    "        F.lit(1).alias(\"is_fraud\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Training labels ready. Row count: {fraud_training_df.count()}\")\n",
    "\n",
    "# Teach the model what \"Good\" behavior looks like \n",
    "# (random transactions from accounts that are NOT in the fraud list)\n",
    "non_fraud_training_df = (\n",
    "    raw_transactions_df\n",
    "    .join(labels_df, on=\"account_id\", how=\"left_anti\") # \"Left Anti\" = Exclude fraud accounts\n",
    "    .sample(fraction=0.1) # Take a 10% random sample\n",
    "    .select(\n",
    "        F.col(\"account_id\"),\n",
    "        F.col(\"transaction_timestamp\").cast(TimestampType()),\n",
    "        F.lit(0).alias(\"is_fraud\")\n",
    "    )\n",
    "    .limit(fraud_training_df.count() * 5) # Optional: Limit size to keep balance\n",
    ")\n",
    "\n",
    "# Combine 2 sets\n",
    "final_training_set_df = fraud_training_df.union(non_fraud_training_df)\n",
    "\n",
    "print(f\"Final training set prepared: {final_training_set_df.count()} rows\")\n",
    "display(final_training_set_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "508956f9-7ef6-4f5d-87ee-5d4888ce6fb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create training set (clean timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79eb1bc7-650a-49be-ab9b-ed5e91f42a93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "training_set = fe.create_training_set(\n",
    "    df=final_training_set_df,\n",
    "    feature_lookups=[\n",
    "        FeatureLookup(\n",
    "            table_name=feature_table_name,\n",
    "            lookup_key=[\"account_id\"],\n",
    "            timestamp_lookup_key=\"transaction_timestamp\"\n",
    "        )\n",
    "    ],\n",
    "    label=\"is_fraud\",\n",
    "    exclude_columns=[\"account_id\", \"transaction_timestamp\"]\n",
    ")\n",
    "\n",
    "training_df = training_set.load_df()\n",
    "\n",
    "# Null check\n",
    "nulls = training_df.filter(F.col(\"avg_txn_7d\").isNull()).count()\n",
    "print(f\"Result: {training_df.count()} rows. {nulls} rows have NULL features.\")\n",
    "\n",
    "display(training_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c76a455-8edd-4806-9b07-8acdb20131fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Proceed to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dda25d4-88cf-401b-b84f-60381f20bf7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-automl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0b4e218-200a-4478-8aa5-55accabe8880",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.automl import classifier\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# STEP 5: Train & Register the Model\n",
    "# -------------------------------------------------------------------------\n",
    "# Drop the keys (account_id, timestamp) to exclude from the model.\n",
    "# Keep the features (avg, stddev, count).\n",
    "\n",
    "train_data = training_df.drop(\"account_id\", \"transaction_timestamp\")\n",
    "\n",
    "# Run AutoML\n",
    "summary = classifier.fit(\n",
    "    dataset=train_data,\n",
    "    target_col=\"is_fraud\",\n",
    "    timeout_minutes=5,\n",
    "    experiment_name=\"/Shared/Apex_Fraud_Experiment\"\n",
    ")\n",
    "\n",
    "print(f\"âœ… Model Trained. Best Trial: {summary.best_trial.model_path}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "feature_store",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
